#!/bin/bash
# AI Model - Unified executable for interacting with AI models via HTTP API
# Based on the Docker Developer Electron app implementation
# Generated by Docker Developer

HISTORY_DIR="$HOME/.docker-developer/history"
MCP_STATE_DIR="$HOME/.docker-developer/mcp"
CACHE_DIR="$HOME/.docker-developer/cache"
mkdir -p "$HISTORY_DIR"
mkdir -p "$MCP_STATE_DIR"
mkdir -p "$CACHE_DIR"

# API endpoint (same as Electron app)
API_URL="http://localhost:12434/engines/llama.cpp/v1/chat/completions"

# MCP Gateway management
MCP_GATEWAY_PID_FILE="$MCP_STATE_DIR/gateway.pid"
MCP_GATEWAY_CONFIG_FILE="$MCP_STATE_DIR/gateway.config"

# Check if MCP gateway is running with correct configuration
is_mcp_gateway_running() {
  if [ -f "$MCP_GATEWAY_PID_FILE" ]; then
    local PID=$(cat "$MCP_GATEWAY_PID_FILE")
    if ps -p "$PID" > /dev/null 2>&1; then
      return 0
    fi
  fi
  return 1
}

# Start MCP gateway with stdio transport (like Electron app)
start_mcp_gateway() {
  local SERVERS="$1"
  local PRIVILEGED="$2"
  
  # Build server arguments
  local SERVER_ARGS=""
  if [ -n "$SERVERS" ]; then
    IFS=',' read -ra SERVER_ARRAY <<< "$SERVERS"
    for server in "${SERVER_ARRAY[@]}"; do
      SERVER_ARGS="$SERVER_ARGS --servers ${server// /}"
    done
  fi
  
  # Build privileged config if needed
  local CONFIG_PATH=""
  if [ -n "$PRIVILEGED" ]; then
    CONFIG_PATH="$MCP_STATE_DIR/privileged-config.yaml"
    
    # Create YAML config for privileged servers
    echo "registry:" > "$CONFIG_PATH"
    IFS=',' read -ra PRIV_ARRAY <<< "$PRIVILEGED"
    for priv_server in "${PRIV_ARRAY[@]}"; do
      cat >> "$CONFIG_PATH" << EOF
  ${priv_server}:
    dockerRunOptions:
      - "--privileged"
      - "-v /var/run/docker.sock:/var/run/docker.sock"
EOF
    done
    
    SERVER_ARGS="$SERVER_ARGS --additional-registry $CONFIG_PATH --additional-config $CONFIG_PATH"
  fi
  
  # Start gateway in background with stdio
  local GATEWAY_CMD="/usr/local/bin/docker mcp gateway run $SERVER_ARGS"
  
  nohup $GATEWAY_CMD > "$MCP_STATE_DIR/gateway.log" 2>&1 &
  local GATEWAY_PID=$!
  echo $GATEWAY_PID > "$MCP_GATEWAY_PID_FILE"
  
  # Save configuration hash
  local CONFIG_HASH=$(echo "${SERVERS}:${PRIVILEGED}" | md5 2>/dev/null || echo "${SERVERS}:${PRIVILEGED}" | md5sum | cut -d' ' -f1)
  echo "$CONFIG_HASH" > "$MCP_GATEWAY_CONFIG_FILE"
  
  # Wait for gateway to be ready (up to 30 seconds)
  local WAITED=0
  while [ $WAITED -lt 30 ]; do
    if grep -q "Initialized in\|Start stdio server" "$MCP_STATE_DIR/gateway.log" 2>/dev/null; then
      return 0
    fi
    sleep 1
    WAITED=$((WAITED + 1))
  done
  
  return 1
}

# Stop MCP gateway
stop_mcp_gateway() {
  if [ -f "$MCP_GATEWAY_PID_FILE" ]; then
    local PID=$(cat "$MCP_GATEWAY_PID_FILE")
    if ps -p "$PID" > /dev/null 2>&1; then
      kill -TERM "$PID" 2>/dev/null
      sleep 1
      if ps -p "$PID" > /dev/null 2>&1; then
        kill -KILL "$PID" 2>/dev/null
      fi
    fi
    rm -f "$MCP_GATEWAY_PID_FILE" "$MCP_GATEWAY_CONFIG_FILE"
  fi
}

# Ensure MCP gateway with correct configuration
ensure_mcp_gateway() {
  local SERVERS="$1"
  local PRIVILEGED="$2"
  local CONFIG_HASH=$(echo "${SERVERS}:${PRIVILEGED}" | md5 2>/dev/null || echo "${SERVERS}:${PRIVILEGED}" | md5sum | cut -d' ' -f1)
  
  # Check if configuration changed
  if [ -f "$MCP_GATEWAY_CONFIG_FILE" ]; then
    local CURRENT_HASH=$(cat "$MCP_GATEWAY_CONFIG_FILE")
    if [ "$CURRENT_HASH" != "$CONFIG_HASH" ]; then
      stop_mcp_gateway
    fi
  fi
  
  # Start if not running
  if ! is_mcp_gateway_running; then
    if ! start_mcp_gateway "$SERVERS" "$PRIVILEGED"; then
      echo "Warning: MCP gateway failed to start" >&2
      return 1
    fi
  fi
  
  return 0
}

# Get context cache key (simplified version of Electron app's caching)
get_cache_key() {
  local MODEL="$1"
  local HISTORY_FILE="$2"
  
  # Generate hash from first few messages (conversation prefix)
  if [ -f "$HISTORY_FILE" ]; then
    local PREFIX_HASH=$(head -n 100 "$HISTORY_FILE" | md5 2>/dev/null || head -n 100 "$HISTORY_FILE" | md5sum | cut -d' ' -f1)
    echo "${MODEL}-${PREFIX_HASH}"
  else
    echo "${MODEL}-empty"
  fi
}

# Make API call with streaming (like Electron app)
make_api_call() {
  local MODEL="$1"
  local CTX_SIZE="$2"
  local MAX_TOKENS="$3"
  local TEMPERATURE="$4"
  local TOP_P="$5"
  local TOP_K="$6"
  local TOOLS="$7"
  local TOOL_CHOICE="$8"
  local DEBUG="$9"
  shift 9
  local HISTORY_FILE="$1"
  shift
  local MCP_SERVERS="$1"
  shift
  local PRIVILEGED_SERVERS="$1"
  shift
  local TOOL_MODE="$1"  # 'native' or 'prompt' (default: prompt)
  
  # Ensure MCP gateway if tools are requested
  if [ -n "$TOOLS" ]; then
    if ! ensure_mcp_gateway "$MCP_SERVERS" "$PRIVILEGED_SERVERS"; then
      echo "Error: Failed to start MCP gateway" >&2
      return 1
    fi
  fi
  
  # Track metrics
  local START_TIME=$(node -e "console.log(Date.now())")
  local FIRST_CHUNK_TIME=""
  local TOTAL_PROMPT_TOKENS=0
  local TOTAL_COMPLETION_TOKENS=0
  local TOTAL_CACHED_TOKENS=0
  local TOOL_CALL_COUNT=0
  
  # Tool execution loop (max 10 iterations like Electron app)
  local MAX_ITERATIONS=10
  local ITERATION=0
  local FINAL_RESPONSE=""
  
  while [ $ITERATION -lt $MAX_ITERATIONS ]; do
    ITERATION=$((ITERATION + 1))
    
    # Build request payload using helper script
    export TOOLS TOOL_MODE TOOL_CHOICE MODEL MAX_TOKENS TEMPERATURE TOP_P CTX_SIZE HISTORY_FILE ITERATION
    local HELPER_DIR="$(cd "$(dirname "$0")" && pwd)"
    local REQUEST_PAYLOAD=$(node "${HELPER_DIR}/ai-model-build-payload.js")
    
    if [ -z "$REQUEST_PAYLOAD" ]; then
      echo "Error: Failed to build request payload" >&2
      return 1
    fi
    
    # Make streaming API call using helper script
    local RESPONSE_FILE=$(mktemp)
    export RESPONSE_FILE ITERATION DEBUG FIRST_CHUNK_TIME START_TIME REQUEST_PAYLOAD TOOL_MODE TOOLS
    
    node "${HELPER_DIR}/ai-model-api-call.js"
    
    local API_EXIT_CODE=$?
    
    # Update first chunk time if it was set
    if [ -f "${RESPONSE_FILE}.firstchunk" ]; then
      FIRST_CHUNK_TIME=$(cat "${RESPONSE_FILE}.firstchunk")
    fi
    
    # Check for API errors
    if [ $API_EXIT_CODE -ne 0 ] || [ -f "${RESPONSE_FILE}.error" ]; then
      if [ -f "${RESPONSE_FILE}.error" ]; then
        local ERROR_DATA=$(cat "${RESPONSE_FILE}.error")
        
        # Check for native mode not supported error
        if [ "$ERROR_DATA" = "NATIVE_MODE_NOT_SUPPORTED" ] && [ "$TOOL_MODE" = "native" ]; then
          echo "" >&2
          echo "‚ùå ERROR: Native tool calling failed" >&2
          echo "" >&2
          echo "Native function calling is not currently supported by:" >&2
          echo "  ‚Ä¢ Docker Desktop's llama.cpp server (requires --jinja flag)" >&2
          echo "  ‚Ä¢ This model (may not be trained for function calling)" >&2
          echo "" >&2
          echo "üí° Solution:" >&2
          echo "  Switch to 'Prompt-Based' tool mode in your executable configuration." >&2
          echo "  This works with any model and achieves the same result." >&2
          echo "" >&2
          echo "To change the configuration:" >&2
          echo "  1. Open the Executables panel in the app" >&2
          echo "  2. Edit this executable" >&2
          echo "  3. Change 'Tool Mode' from 'Native' to 'Prompt-Based'" >&2
          echo "  4. Save" >&2
          echo "" >&2
          echo "Or use: ai-model prompt --tool-mode prompt --tools <tools> <model> \"<prompt>\"" >&2
          echo "" >&2
          
          rm -f "${RESPONSE_FILE}"*
          return 1
        fi
        
        # Check for context size error
        if echo "$ERROR_DATA" | grep -q "context size\|exceed_context_size"; then
          echo "" >&2
          echo "ERROR: Context size exceeded!" >&2
          echo "" >&2
          echo "The request is too large for the current context window." >&2
          echo "" >&2
          echo "Solutions:" >&2
          echo "  1. Clear chat history: ai-model clear $MODEL" >&2
          echo "  2. Increase context size: --ctx-size <larger_value>" >&2
          echo "  3. Use fewer tools or shorter prompts" >&2
          echo "" >&2
          echo "Current context: $CTX_SIZE tokens" >&2
          echo "Recommended: --ctx-size $((CTX_SIZE * 2))" >&2
        else
          echo "Error: API request failed: $ERROR_DATA" >&2
        fi
      else
        echo "Error: API request failed" >&2
      fi
      
      rm -f "${RESPONSE_FILE}"*
      return 1
    fi
    
    # Load response data
    if [ ! -f "${RESPONSE_FILE}.json" ]; then
      echo "Error: No response from API" >&2
      rm -f "${RESPONSE_FILE}"*
      return 1
    fi
    
    local RESPONSE_JSON=$(cat "${RESPONSE_FILE}.json")
    local RESPONSE_CONTENT=$(cat "${RESPONSE_FILE}.content" 2>/dev/null || echo "")
    
    # Parse response and check for tool calls using helper script
    export RESPONSE_JSON RESPONSE_CONTENT HISTORY_FILE TOOL_MODE RESPONSE_FILE
    local HAS_TOOL_CALLS=$(node "${HELPER_DIR}/ai-model-check-tools.js")
    
    # If tool calls were detected, erase the JSON block from terminal
    if [ "$HAS_TOOL_CALLS" = "true" ] && [ "$ITERATION" -eq 1 ]; then
      # Count lines in the response to erase
      local LINE_COUNT=$(echo "$RESPONSE_CONTENT" | wc -l)
      
      # Get terminal width (default to 80 if not set)
      local TERM_WIDTH=${COLUMNS:-80}
      
      # Create a line of spaces to overwrite content
      local SPACES=$(printf "%${TERM_WIDTH}s" "")
      
      # Move cursor up and overwrite each line with spaces
      for ((i=0; i<LINE_COUNT+1; i++)); do
        printf "\033[A"          # Move cursor up one line
        printf "\r%s\r" "$SPACES" # Overwrite with spaces and return to start
      done
    fi
    
    # Accumulate token usage
    local USAGE=$(echo "$RESPONSE_JSON" | node -e "const d=JSON.parse(require('fs').readFileSync(0,'utf8'));console.log(JSON.stringify(d.usage||{}))")
    if [ -n "$USAGE" ]; then
      local PROMPT_TOKENS=$(echo "$USAGE" | node -e "const u=JSON.parse(require('fs').readFileSync(0,'utf8'));console.log(u.prompt_tokens||0)")
      local COMPLETION_TOKENS=$(echo "$USAGE" | node -e "const u=JSON.parse(require('fs').readFileSync(0,'utf8'));console.log(u.completion_tokens||0)")
      local CACHED_TOKENS=$(echo "$USAGE" | node -e "const u=JSON.parse(require('fs').readFileSync(0,'utf8'));console.log(u.prompt_tokens_details&&u.prompt_tokens_details.cached_tokens||0)")
      
      TOTAL_PROMPT_TOKENS=$((TOTAL_PROMPT_TOKENS + PROMPT_TOKENS))
      TOTAL_COMPLETION_TOKENS=$((TOTAL_COMPLETION_TOKENS + COMPLETION_TOKENS))
      TOTAL_CACHED_TOKENS=$((TOTAL_CACHED_TOKENS + CACHED_TOKENS))
    fi
    
    # Handle tool calls
    if [ "$HAS_TOOL_CALLS" = "true" ]; then
      TOOL_CALL_COUNT=$((TOOL_CALL_COUNT + 1))
      
      if [ "$DEBUG" = "true" ]; then
        echo "" >&2
        echo "[Executing tools... (iteration $ITERATION)]" >&2
      fi
      
      # Execute tool calls using helper script
      export RESPONSE_FILE HISTORY_FILE DEBUG TOOL_MODE
      node "${HELPER_DIR}/ai-model-execute-tools.js"
      
      # Clean up temp files and continue loop
      rm -f "${RESPONSE_FILE}"*
      continue
    else
      # No tool calls - this is the final response
      FINAL_RESPONSE="$RESPONSE_CONTENT"
      rm -f "${RESPONSE_FILE}"*
      break
    fi
  done
  
  # Everything is streamed in real-time
  # No additional printing needed
  
  # Show debug metrics (like Electron app)
  if [ "$DEBUG" = "true" ]; then
    local END_TIME=$(node -e "console.log(Date.now())")
    local TOTAL_TOKENS=$((TOTAL_PROMPT_TOKENS + TOTAL_COMPLETION_TOKENS))
    local TOTAL_DURATION=$((END_TIME - START_TIME))
    
    # Calculate context usage
    local CONTEXT_PCT=0
    if [ "$CTX_SIZE" -gt 0 ] && [ $TOTAL_PROMPT_TOKENS -gt 0 ]; then
      CONTEXT_PCT=$(node -e "console.log(((${TOTAL_PROMPT_TOKENS} / ${CTX_SIZE}) * 100).toFixed(1))")
    fi
    
    # Format durations
    local LATENCY_MS=0
    if [ -n "$FIRST_CHUNK_TIME" ]; then
      LATENCY_MS=$((FIRST_CHUNK_TIME - START_TIME))
    fi
    
    echo "" >&2
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó" >&2
    echo "‚ïë  AGENT RESPONSE SUMMARY (Debug Mode)" >&2
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù" >&2
    echo "‚ïë  Performance:" >&2
    echo "‚ïë    Latency: ${LATENCY_MS}ms (time to first token)" >&2
    echo "‚ïë    Total Time: ${TOTAL_DURATION}ms" >&2
    echo "‚ïë  Token Usage:" >&2
    echo "‚ïë    Prompt Tokens: ${TOTAL_PROMPT_TOKENS}" >&2
    echo "‚ïë    Cached Tokens: ${TOTAL_CACHED_TOKENS}" >&2
    echo "‚ïë    Completion Tokens: ${TOTAL_COMPLETION_TOKENS}" >&2
    echo "‚ïë    Total Tokens: ${TOTAL_TOKENS}" >&2
    if [ "$CTX_SIZE" -gt 0 ]; then
      echo "‚ïë    Context Used: ${CONTEXT_PCT}% of ${CTX_SIZE} tokens" >&2
    fi
    if [ $TOOL_CALL_COUNT -gt 0 ]; then
      echo "‚ïë  Tool Calls: ${TOOL_CALL_COUNT}" >&2
    fi
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù" >&2
    echo "" >&2
  fi
}

# Parse command
COMMAND="${1:-help}"
shift || true

case "$COMMAND" in
  clear)
    if [ -z "$1" ]; then
      # Clear all JSON files in history directory and subdirectories
      find "$HISTORY_DIR" -type f -name "*.json" -delete 2>/dev/null
      rm -rf "$CACHE_DIR"/* 2>/dev/null
      echo "‚úì Cleared all model chat histories and cache"
    else
      MODEL="$1"
      HISTORY_FILE="$HISTORY_DIR/${MODEL}.json"
      if [ -f "$HISTORY_FILE" ]; then
        rm "$HISTORY_FILE"
        echo "‚úì Cleared chat history for $MODEL"
      else
        echo "No chat history found for $MODEL"
      fi
    fi
    ;;
    
  status)
    MODEL="$1"
    if [ -z "$MODEL" ]; then
      echo "Usage: ai-model status <model>"
      exit 1
    fi
    
    HISTORY_FILE="$HISTORY_DIR/${MODEL}.json"
    if [ -f "$HISTORY_FILE" ]; then
      export HISTORY_FILE
      MSG_COUNT=$(node -e "const h=JSON.parse(require('fs').readFileSync(process.env.HISTORY_FILE,'utf8'));console.log(h.length)")
      
      echo "Model: $MODEL"
      echo "Messages in history: $MSG_COUNT"
      echo "History file: $HISTORY_FILE"
    else
      echo "No chat history for $MODEL"
    fi
    ;;
    
  prompt)
    # Parse options
    CTX_SIZE=8192
    MAX_TOKENS=2048
    TEMPERATURE=0.7
    TOP_P=0.9
    TOP_K=40
    TOOLS=""
    TOOL_CHOICE="auto"
    TOOL_MODE="prompt"
    DEBUG=false
    MCP_SERVERS=""
    PRIVILEGED_SERVERS=""
    
    while [ $# -gt 0 ]; do
      case "$1" in
        --ctx-size) CTX_SIZE="$2"; shift 2 ;;
        --max-tokens) MAX_TOKENS="$2"; shift 2 ;;
        --temperature) TEMPERATURE="$2"; shift 2 ;;
        --top_p) TOP_P="$2"; shift 2 ;;
        --top_k) TOP_K="$2"; shift 2 ;;
        --tools) TOOLS="$2"; shift 2 ;;
        --tool-choice) TOOL_CHOICE="$2"; shift 2 ;;
        --tool-mode) TOOL_MODE="$2"; shift 2 ;;
        --debug) DEBUG="$2"; shift 2 ;;
        --mcp-servers) MCP_SERVERS="$2"; shift 2 ;;
        --privileged) PRIVILEGED_SERVERS="$2"; shift 2 ;;
        *) break ;;
      esac
    done
    
    MODEL="$1"
    shift
    PROMPT="$*"
    
    if [ -z "$MODEL" ]; then
      echo "Usage: ai-model prompt [options] <model> <prompt>"
      exit 1
    fi
    
    # Read from stdin if no prompt
    if [ -z "$PROMPT" ]; then
      PROMPT=$(cat)
    fi
    
    HISTORY_FILE="$HISTORY_DIR/${MODEL}.json"
    
    # Add user message to history (and system message if first message)
    export PROMPT HISTORY_FILE
    HELPER_DIR="$(cd "$(dirname "$0")" && pwd)"
    node "${HELPER_DIR}/ai-model-add-user-msg.js"
    
    # Make API call
    export MODEL CTX_SIZE MAX_TOKENS TEMPERATURE TOP_P TOP_K TOOLS TOOL_CHOICE DEBUG HISTORY_FILE MCP_SERVERS PRIVILEGED_SERVERS TOOL_MODE
    make_api_call "$MODEL" "$CTX_SIZE" "$MAX_TOKENS" "$TEMPERATURE" "$TOP_P" "$TOP_K" "$TOOLS" "$TOOL_CHOICE" "$DEBUG" "$HISTORY_FILE" "$MCP_SERVERS" "$PRIVILEGED_SERVERS" "$TOOL_MODE"
    ;;
    
  run)
    # Interactive mode (same options as prompt)
    CTX_SIZE=8192
    MAX_TOKENS=2048
    TEMPERATURE=0.7
    TOP_P=0.9
    TOP_K=40
    TOOLS=""
    TOOL_CHOICE="auto"
    TOOL_MODE="prompt"
    DEBUG=false
    MCP_SERVERS=""
    PRIVILEGED_SERVERS=""
    
    while [ $# -gt 0 ]; do
      case "$1" in
        --ctx-size) CTX_SIZE="$2"; shift 2 ;;
        --max-tokens) MAX_TOKENS="$2"; shift 2 ;;
        --temperature) TEMPERATURE="$2"; shift 2 ;;
        --top_p) TOP_P="$2"; shift 2 ;;
        --top_k) TOP_K="$2"; shift 2 ;;
        --tools) TOOLS="$2"; shift 2 ;;
        --tool-choice) TOOL_CHOICE="$2"; shift 2 ;;
        --tool-mode) TOOL_MODE="$2"; shift 2 ;;
        --debug) DEBUG="$2"; shift 2 ;;
        --mcp-servers) MCP_SERVERS="$2"; shift 2 ;;
        --privileged) PRIVILEGED_SERVERS="$2"; shift 2 ;;
        *) break ;;
      esac
    done
    
    MODEL="$1"
    
    if [ -z "$MODEL" ]; then
      echo "Usage: ai-model run [options] <model>"
      exit 1
    fi
    
    HISTORY_FILE="$HISTORY_DIR/${MODEL}.json"
    
    # Show banner
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
    echo "‚ïë  Interactive Agent Chat - $MODEL"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo "‚ïë  Context: $CTX_SIZE tokens | Max Output: $MAX_TOKENS tokens"
    if [ -n "$TOOLS" ]; then
      echo "‚ïë  Tools: $TOOLS"
    fi
    echo "‚ïë  Press Ctrl+C to exit"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo ""
    
    trap 'echo ""; echo "Goodbye!"; exit 0' INT
    
    while true; do
      printf "\n\033[1;36mYou:\033[0m "
      read -r USER_INPUT
      
      if [ -z "$USER_INPUT" ]; then
        continue
      fi
      
      # Add to history (and system message if first message)
      export PROMPT="$USER_INPUT" HISTORY_FILE
      HELPER_DIR="$(cd "$(dirname "$0")" && pwd)"
      node "${HELPER_DIR}/ai-model-add-user-msg.js"
      
      printf "\033[1;32m$MODEL:\033[0m "
      export MODEL CTX_SIZE MAX_TOKENS TEMPERATURE TOP_P TOP_K TOOLS TOOL_CHOICE DEBUG HISTORY_FILE MCP_SERVERS PRIVILEGED_SERVERS TOOL_MODE
      make_api_call "$MODEL" "$CTX_SIZE" "$MAX_TOKENS" "$TEMPERATURE" "$TOP_P" "$TOP_K" "$TOOLS" "$TOOL_CHOICE" "$DEBUG" "$HISTORY_FILE" "$MCP_SERVERS" "$PRIVILEGED_SERVERS" "$TOOL_MODE"
      echo ""
    done
    ;;
    
  help|--help|-h|*)
    cat << 'HELPTEXT'
ai-model - Unified AI model interaction tool (HTTP API)

USAGE:
    ai-model <command> [options]

COMMANDS:
    clear               Clear all model chat histories
    clear <model>       Clear chat history for specific model
    
    prompt [opts] <model> <prompt>
                        Execute a single prompt
    
    run [opts] <model>  Run interactive chat loop
    
    status <model>      Show model chat history status

OPTIONS (for prompt/run):
    --ctx-size <n>          Context window size (default: 8192)
    --max-tokens <n>        Maximum tokens in response (default: 2048)
    --temperature <n>       Temperature 0-1 (default: 0.7)
    --top_p <n>             Top P sampling (default: 0.9)
    --top_k <n>             Top K sampling (default: 40)
    --tools <list>          Comma-separated tool names
    --tool-choice <choice>  Tool choice mode (default: auto)
    --tool-mode <mode>      Tool calling mode: 'native' or 'prompt' (default: prompt)
                            - native: Fast, uses OpenAI function calling format
                            - prompt: Compatible, works with any model
    --mcp-servers <list>    Comma-separated MCP server names
    --privileged <list>     Comma-separated privileged server names
    --debug <true|false>    Enable debug mode (default: false)

FEATURES:
    ‚úì Streaming responses (like Electron app)
    ‚úì Context caching for faster responses
    ‚úì MCP tool integration with automatic execution
    ‚úì Conversation history management
    ‚úì Token usage tracking
    ‚úì Detailed error messages

EXAMPLES:
    # Basic usage
    ai-model prompt gpt-4 "Hello!"
    ai-model run gpt-4
    
    # With tools (prompt-based, works with any model)
    ai-model prompt --tools get_time gpt-4 "What time is it?"
    
    # With tools (native mode, faster if supported)
    ai-model prompt --tools get_time --tool-mode native gpt-4 "What time is it?"
    
    # With larger context
    ai-model prompt --ctx-size 16384 gpt-4 "Long prompt..."
    
    # Debug mode
    ai-model prompt --debug true gpt-4 "Test"
    
    # Clear history
    ai-model clear gpt-4

HELPTEXT
    ;;
esac
